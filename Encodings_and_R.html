<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta http-equiv="content-type"
 content="text/html; charset=ISO-8859-1">
  <title>Encodings and R</title>
</head>
<body>
<h1 align="center">Encodings and R</h1>
<p>
The use of encodings is raised sporadically on the R mailing lists,
with discussion of ideas to `do better'.&nbsp; R has been developed by
authors speaking English or a Western European language, and its
current mindset is the ISO Latin 1 (aka ISO 8859-1) character
set.&nbsp; Even these authors find some problems, for example the lack
of&nbsp; some currency symbols (notably the Euro, &#8364; if it displays for
you).&nbsp; Users of R in Eastern Europe need more characters and are
sometimes puzzled that Latin 2 (aka ISO 8859-2) sometimes works.&nbsp;
Other languages present much greater challenges, and there is a project
to `Japanize' R which (for obvious reasons) is little known outside
Japan.
</p>
<p>One of the challenges is that in European usage, <tt>nchar(x)</tt>
is
the number of characters in the string but is frequently used for <br>
adjusting layouts.&nbsp; In other encodings there can be different
values for<br>
</p>
<ol>
  <li>The number of characters in a string</li>
  <li>The number of bytes used to store a string and</li>
  <li>The number of columns used to display the string<br>
  </li>
</ol>
Fortunately <tt>nchar</tt> is little used (and often to see if a
string is empty), but the C-level equivalents are widely used in all
three meanings.<br>
<h2>Encoding in R 1.8.x</h2>
<p>The default behavour&nbsp; is to treat characters as a stream of
8-bit
bytes, and not to interpret them other than to assume that each byte
represents one character.&nbsp; The only exceptions are&lt;\p&gt;
</p>
<ul>
  <li>The connections mechanism allows the remapping of input files to
the `native' encoding.&nbsp; Since the encoding defaults to <span
 style="font-family: monospace;">getOption("encoding")</span> it is
possible to use this within <span style="font-family: monospace;">read.table</span>
fairly easily.&nbsp; Note that this is a byte-level remapping, and that
not all of R's input goes through the connections mechanism.</li>
  <li>Those graphical devices which name glyphs, notably <span
 style="font-family: monospace;">postscript</span> and <span
 style="font-family: monospace;">pdf</span>, do have to deal with
encoding, and they allow the user to specify the byte-levle mapping of
code to glyphs.&nbsp; This has been one of the problem areas as the
standard Adobe font metrics included in R only cover ISO Latin 1 and
not for example the Euro (although the URW font metrics supplied do
have it).&nbsp; Similarly, the Adobe fonts do not cover all of ISO
Latin 2.</li>
</ul>
With these exceptions, character encoding is the responsibility of the
environment provided by the OS, so<br>
<ul>
  <li>What glyph is displayed on a graphics device depends on the
encoding of the font selected.&nbsp; <br>
  </li>
  <li>How output is displayed in the terminal depends on the font and
locale selected.<br>
  </li>
  <li>What numeric code is generated by keystrokes depends on the
keyboard mapping or locale in use.<br>
  </li>
</ul>
<h2>Towards Unicode?</h2>
<p>
It seems generally agreed that Unicode is the way to cope with all
known character sets.&nbsp; There is a comprehensive <a
 href="http://www.cl.cam.ac.uk/%7Emgk25/unicode.html">FAQ</a>, rather
Unix-oriented. Unicode defines a numbering of characters up to 31 bits
although it seems agreed than only 21 bits will ever be used.&nbsp;
However, to use it as an encoding would be rather wasteful, and most
people seem to use UTF-8, in which each character is represented as
1,2,...,6 bytes (and how many can be deduced from the first
byte).&nbsp; As 7-bit ASCII characters are represented as a single byte
(with the high bit zero) there is no storage overhead unless
non-American characters are used.&nbsp; However, not only can a single
character be stored in a variable number of bytes but it can be
displayed in 1, 2 or even 0 columns.</p>
<p>
Linux and other systems based on <span style="font-family: monospace;">glibc</span>
are moving towards UTF-8 support: if the locale is set to <span
 style="font-family: monospace;">en_GB.utf8</span> then the run-time
assumes UTF-8 encoding is required.&nbsp; Major Unix distributions
(e.g. Solaris 2.8) are also incorporating UTF-8 support.</p>
<p>
The position on Windows is less clear. Windows has long supported `wide
characters', that is 2-byte representations of characters, and provides
fonts covering a very wide range of glyphs (at least under NT-based
versions of Windows).&nbsp; It is not clear that CJK versions of
Windows use a 2-byte encoding that is consistent with Unicode, and it
seems there is no support for UTF-8.</p>
<h2>Implementation issues</h2>
If R were to use UTF-8 internally we would need to handle at least the
following issues<br>
<ul>
  <li>Conversion to UTF-8 on input.&nbsp; This would be easy for
connections-based input (although a more general way to describe the
source encoding would be required), but all the console/keyboard-based
input routines would need to be modified.</li>
  <li>Conversion of text output.&nbsp; This would be easy for
connections-based output, but dialog-box based output would need to be
handled, for example.&nbsp; It is not clear what to do with characters
which cannot be mapped -- the graphical devices currently map to space.<br>
  </li>
  <li>Handling of file names.&nbsp; It is quite common to read,
manipulate and process file names.&nbsp; If these are allowed to be
UTF-8 this would be straightforward, but are they?&nbsp; Probably
usually not.<br>
  </li>
  <li>Graphical text output.&nbsp; This boils down to either selecting
suitable fonts or converting to the encoding of the fonts.&nbsp; I
suspect that under Windows a 2-byte encoding would be used, and X
servers can make use of&nbsp; ISO10646-1 fonts via UTF-8.</li>
  <li>Text manipulation, for example <span
 style="font-family: monospace;">match</span> and <span
 style="font-family: monospace;">grep</span> and <span
 style="font-family: monospace;">tolower</span>.&nbsp; For some of
these UTF-8 versions are readily available and for others we would have
to rewrite.&nbsp; And a lucky few like <span
 style="font-family: monospace;">match</span> would work directly on
the encoded strings.&nbsp; String collation is also an issue (in a few
places).&nbsp; Most widespread is the use of <span
 style="font-family: monospace;">snprintf</span>, <span
 style="font-family: monospace;">strncmp</span> and simple loops to
e.g, map <span style="font-family: monospace;">\</span> to <span
 style="font-family: monospace;">/</span>. <br>
  </li>
</ul>
The API for extending R would be problematical.&nbsp; There are a few
hundred R extensions written in C and FORTRAN, and a few of them
manipulate character vectors.&nbsp; They would not be expecting UTF-8
encoding (and probably have not thought about encodings at all).&nbsp;
Possible ways forward are<br>
<ul>
  <li>To map to an 8-bit encoding (Latin1?) and back again when .C does
the copying.</li>
  <li>Just to pass through the stream of bytes.</li>
</ul>
This does raise the issue of whether the <span
 style="font-family: monospace;">CHAR</span> internal type should be
used for UTF-8 or a new type created.&nbsp; It would probably be better
to create a new type for raw bytes.<br>
<br>
Brian Ripley<br>
2004-01-09<br>
<br>
<p>
</p>
</body>
</html>
